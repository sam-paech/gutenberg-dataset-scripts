{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import tempfile\n",
    "import chardet\n",
    "from typing import Dict\n",
    "\n",
    "# ----------------------------\n",
    "# Function Definitions\n",
    "# ----------------------------\n",
    "\n",
    "def parse_gutenberg_header(header_string: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Parses a Project Gutenberg header string to extract the title, author, and note.\n",
    "\n",
    "    Parameters:\n",
    "        header_string (str): The header string to parse.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, str]: A dictionary containing the title, author, and note.\n",
    "    \"\"\"\n",
    "    # Initialize default values\n",
    "    title = \"\"\n",
    "    author = \"Unknown author\"\n",
    "    note = \"\"\n",
    "\n",
    "    # Step 1: Normalize the string by replacing line breaks with spaces\n",
    "    normalized = ' '.join(header_string.splitlines())\n",
    "    normalized = re.sub(r'\\s+', ' ', normalized).strip()\n",
    "\n",
    "    # Step 2: Remove known prefixes\n",
    "    prefixes = [\n",
    "        r'^The Project Gutenberg EBook of ',\n",
    "        r'^The Project Gutenberg EBook ',\n",
    "        r'^Project Gutenberg\\'s ',\n",
    "        r'^The Project Gutenberg Etext of ',\n",
    "        r'^The Project Gutenberg Etext ',\n",
    "        r'^Project Gutenberg Etext of',  \n",
    "        r'^\\*\\*\\*The Project Gutenberg Etext of ',\n",
    "        r'^\\*\\*\\*The Project Gutenberg eText of ',\n",
    "        r'^\\*\\*\\*The Project Gutenberg Etext of ',\n",
    "        r'^\\*\\*The Project Gutenberg Etext of ',\n",
    "        r'^The Project Gutenberg EBook: ',\n",
    "        r'^Project Gutenberg Etext ',\n",
    "        r'^Project Gutenberg ',\n",
    "        r'^The Project Gutenberg eBook, ',\n",
    "\n",
    "    ]\n",
    "    prefix_removed = False\n",
    "    for prefix in prefixes:\n",
    "        match = re.match(prefix, normalized, re.IGNORECASE)\n",
    "        if match:\n",
    "            normalized = re.sub(prefix, '', normalized, flags=re.IGNORECASE)\n",
    "            prefix_removed = True\n",
    "            break  # Assume only one prefix applies\n",
    "\n",
    "    if not prefix_removed:\n",
    "        # If no known prefix is found, proceed without removal\n",
    "        pass\n",
    "\n",
    "    # Step 3: Extract note\n",
    "    # 3a. Check for note in parentheses\n",
    "    parenthetical_note = re.search(r'\\(([^)]+)\\)', normalized)\n",
    "    if parenthetical_note:\n",
    "        note = parenthetical_note.group(1).strip()\n",
    "        # Remove the parenthetical note from normalized\n",
    "        normalized = re.sub(r'\\([^)]*\\)', '', normalized).strip()\n",
    "\n",
    "    # 3b. Check for note with #number in our series\n",
    "    hash_note_match = re.search(r'#\\d+\\s+in our series(?: of [^,]+)?(?: by [^,]+)?', normalized, re.IGNORECASE)\n",
    "    if hash_note_match:\n",
    "        # If note was already captured in parentheses, append\n",
    "        if note:\n",
    "            note += ' ' + hash_note_match.group(0).strip()\n",
    "        else:\n",
    "            note = hash_note_match.group(0).strip()\n",
    "        # Remove the hash note from normalized\n",
    "        normalized = re.sub(r'#\\d+\\s+in our series(?: of [^,]+)?(?: by [^,]+)?', '', normalized).strip()\n",
    "\n",
    "    # 3c. Check for other notes (e.g., \"Plagiarized from ...\")\n",
    "    other_note_match = re.search(r'(Plagiarized from [^,]+(?:, [^,]+)*)', normalized, re.IGNORECASE)\n",
    "    if other_note_match:\n",
    "        if note:\n",
    "            note += ' ' + other_note_match.group(1).strip()\n",
    "        else:\n",
    "            note = other_note_match.group(1).strip()\n",
    "        # Remove the other note from normalized\n",
    "        normalized = re.sub(r'Plagiarized from [^,]+(?:, [^,]+)*', '', normalized, flags=re.IGNORECASE).strip()\n",
    "\n",
    "    # Step 4: Extract author\n",
    "    # 4a. Look for \", by [author]\"\n",
    "    by_author_match = re.search(r',\\s*by\\s+([^,]+)', normalized, re.IGNORECASE)\n",
    "    if by_author_match:\n",
    "        author = by_author_match.group(1).strip()\n",
    "        # Remove the author part from normalized\n",
    "        normalized = re.sub(r',\\s*by\\s+[^,]+', '', normalized).strip()\n",
    "    else:\n",
    "        # 4b. Look for \" by [author]\" without preceding comma\n",
    "        by_author_match = re.search(r'\\sby\\s+([^,]+)', normalized, re.IGNORECASE)\n",
    "        if by_author_match:\n",
    "            author = by_author_match.group(1).strip()\n",
    "            # Remove the author part from normalized\n",
    "            normalized = re.sub(r'\\sby\\s+[^,]+', '', normalized).strip()\n",
    "\n",
    "    # Step 5: The remaining text is the title\n",
    "    # Remove any trailing commas or version info\n",
    "    title = normalized.strip().rstrip(',')\n",
    "\n",
    "    # Final cleanup: Remove version info like \", v2\" or \", v10\"\n",
    "    title = re.sub(r',\\s*v\\d+(\\.\\d+)?$', '', title, flags=re.IGNORECASE).strip()\n",
    "\n",
    "    return {\n",
    "        'title': title,\n",
    "        'author': author,\n",
    "        'note': note\n",
    "    }\n",
    "\n",
    "def find_zip_files(source_dir: Path):\n",
    "    \"\"\"\n",
    "    Recursively find all zip files in the given source directory.\n",
    "\n",
    "    Parameters:\n",
    "        source_dir (Path): The source directory path.\n",
    "\n",
    "    Returns:\n",
    "        List[Path]: A list of paths to zip files.\n",
    "    \"\"\"\n",
    "    zip_files = list(source_dir.rglob(\"*.zip\"))\n",
    "    return zip_files\n",
    "\n",
    "def load_results(results_file: Path):\n",
    "    \"\"\"\n",
    "    Load existing results from the JSON file if it exists.\n",
    "\n",
    "    Parameters:\n",
    "        results_file (Path): Path to the results JSON file.\n",
    "\n",
    "    Returns:\n",
    "        Dict: A dictionary of existing results.\n",
    "    \"\"\"\n",
    "    if results_file.exists():\n",
    "        try:\n",
    "            with open(results_file, 'r', encoding='utf-8') as f:\n",
    "                results = json.load(f)\n",
    "            print(f\"Loaded {len(results)} existing results.\")\n",
    "            return results\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Results file is corrupted. Starting fresh.\")\n",
    "            return {}\n",
    "    else:\n",
    "        print(\"No existing results found. Starting fresh.\")\n",
    "        return {}\n",
    "\n",
    "def save_results(results: Dict, results_file: Path):\n",
    "    \"\"\"\n",
    "    Save the results dictionary to the JSON file.\n",
    "\n",
    "    Parameters:\n",
    "        results (Dict): The results dictionary.\n",
    "        results_file (Path): Path to the results JSON file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(results_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "        #print(f\"Saved {len(results)} results to {results_file}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results to '{results_file}': {e}\")\n",
    "\n",
    "def handle_duplicate(target_dir: Path, base_name: str, extension: str = \".zip\"):\n",
    "    \"\"\"\n",
    "    Handle duplicate filenames by appending a counter.\n",
    "\n",
    "    Parameters:\n",
    "        target_dir (Path): The target directory.\n",
    "        base_name (str): The base name for the new file.\n",
    "        extension (str): The file extension.\n",
    "\n",
    "    Returns:\n",
    "        str: A unique filename with the extension.\n",
    "    \"\"\"\n",
    "    candidate = target_dir / f\"{base_name}{extension}\"\n",
    "    counter = 1\n",
    "    while candidate.exists():\n",
    "        candidate = target_dir / f\"{base_name} ({counter}){extension}\"\n",
    "        counter += 1\n",
    "    return candidate.name  # Return only the filename, not the path\n",
    "\n",
    "def detect_encoding(file_path: Path):\n",
    "    \"\"\"\n",
    "    Detect the encoding of a file using chardet.\n",
    "    Prioritize 'utf-8' and 'latin-1' to handle a wide range of characters.\n",
    "    Avoid using 'ascii' and 'charmap' as they are too limited.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (Path): Path to the text file.\n",
    "\n",
    "    Returns:\n",
    "        str: Detected encoding or a reliable fallback.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            raw_data = f.read(100000)  # Read first 100KB for detection\n",
    "        result = chardet.detect(raw_data)\n",
    "        encoding = result['encoding']\n",
    "        confidence = result['confidence']\n",
    "        if encoding:\n",
    "            encoding_lower = encoding.lower()\n",
    "            if encoding_lower in ['ascii', 'charmap']:\n",
    "                #print(f\"Detected encoding '{encoding}' with confidence {confidence:.2f} for '{file_path}'. Falling back to 'latin-1'.\")\n",
    "                return 'latin-1'\n",
    "            if confidence >= 0.5:\n",
    "                #print(f\"Detected encoding '{encoding}' with confidence {confidence:.2f} for '{file_path}'.\")\n",
    "                return encoding\n",
    "            else:\n",
    "                #print(f\"Low confidence ({confidence:.2f}) for encoding '{encoding}' in '{file_path}'. Falling back to 'utf-8'.\")\n",
    "                pass\n",
    "        else:\n",
    "            #print(f\"No encoding detected for '{file_path}'. Falling back to 'utf-8'.\")\n",
    "            pass\n",
    "        return 'utf-8'  # More reliable fallback\n",
    "    except Exception as e:\n",
    "        print(f\"Error detecting encoding for '{file_path}': {e}. Falling back to 'utf-8'.\")\n",
    "        return 'utf-8'  # Fallback encoding\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def sanitize_filename(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove or replace characters that are invalid in filenames.\n",
    "    \n",
    "    This function:\n",
    "    1. Removes control characters\n",
    "    2. Removes reserved characters (/, \\, ?, %, *, :, |, \", <, >, .)\n",
    "    3. Removes leading/trailing spaces and dots\n",
    "    4. Replaces spaces with underscores\n",
    "    5. Limits the filename length to 255 characters\n",
    "    6. Ensures the filename is not empty\n",
    "    \n",
    "    Parameters:\n",
    "        name (str): The original filename.\n",
    "    \n",
    "    Returns:\n",
    "        str: Sanitized filename.\n",
    "    \"\"\"\n",
    "    # Normalize unicode characters\n",
    "    name = unicodedata.normalize('NFKD', name).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    \n",
    "    # Remove control characters\n",
    "    name = ''.join(c for c in name if ord(c) > 31 and ord(c) != 127)\n",
    "    \n",
    "    # Remove reserved characters\n",
    "    name = re.sub(r'[/\\\\?%*:|\"<>.,;=]', '', name)\n",
    "    \n",
    "    # Replace spaces with underscores\n",
    "    name = name.replace(' ', '_')\n",
    "    \n",
    "    # Remove leading/trailing spaces and dots\n",
    "    name = name.strip('. ')\n",
    "    \n",
    "    # Limit length to 255 characters\n",
    "    name = name[:255]\n",
    "    \n",
    "    # Ensure the filename is not empty\n",
    "    if not name:\n",
    "        name = \"unnamed_file\"\n",
    "    \n",
    "    return name\n",
    "\n",
    "def process_zip_file(zip_path: Path, target_dir: Path, results: Dict, results_file: Path, save_every: int, save_counter: Dict):\n",
    "    \"\"\"\n",
    "    Process a single zip file:\n",
    "    - Extract it\n",
    "    - If it contains a single text file, parse details using parse_gutenberg_header\n",
    "    - Clean up extracted files\n",
    "    - Copy and rename the zip file\n",
    "    - Update results\n",
    "    - Save results every 'save_every' items\n",
    "\n",
    "    Parameters:\n",
    "        zip_path (Path): Path to the zip file.\n",
    "        target_dir (Path): Path to the target directory.\n",
    "        results (Dict): The results dictionary.\n",
    "        results_file (Path): Path to the results JSON file.\n",
    "        save_every (int): Number of items after which to save results.\n",
    "        save_counter (Dict): A dictionary to keep track of the number of items processed since last save.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if processed successfully, False otherwise.\n",
    "    \"\"\"\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        try:\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(tmpdirname)\n",
    "        except zipfile.BadZipFile:\n",
    "            print(f\"Skipping '{zip_path}': Not a valid zip file.\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting '{zip_path}': {e}\")\n",
    "            return False\n",
    "\n",
    "        extracted_files = list(Path(tmpdirname).rglob(\"*\"))\n",
    "        text_files = [f for f in extracted_files if f.is_file() and f.suffix.lower() in ['.txt', '.md', '.text']]\n",
    "\n",
    "        if len(text_files) != 1:\n",
    "            #print(f\"Skipping '{zip_path}': Expected 1 text file, found {len(text_files)}.\")\n",
    "            return False  # Not a success\n",
    "\n",
    "        text_file = text_files[0]\n",
    "\n",
    "        # Detect encoding\n",
    "        encoding = detect_encoding(text_file)\n",
    "        try:\n",
    "            with open(text_file, 'r', encoding=encoding, errors='replace') as f:\n",
    "                content = f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading '{text_file}': {e}\")\n",
    "            return False\n",
    "\n",
    "        to_check = content[:10000].split('\\n')\n",
    "        if 'Language: English' not in to_check:\n",
    "            return False\n",
    "        # Parse header using parse_gutenberg_header\n",
    "        sections = content.split(\"\\n\\n\", 1)        \n",
    "        header_string = sections[0] if len(sections) > 0 else \"\"\n",
    "        if header_string.strip() == '**This is a COPYRIGHTED Project Gutenberg Etext, Details Below**':\n",
    "            #header_string = sections[1] if len(sections) > 1 else \"\"\n",
    "            #print(sections[:100])\n",
    "            return False\n",
    "        details = parse_gutenberg_header(header_string)        \n",
    "\n",
    "        if not details.get('title'):\n",
    "            print(f\"Skipping '{zip_path}': Title not found in details.\")\n",
    "            print(f\"Extracted Top Section:\\n{header_string}\\n{'-'*40}\")\n",
    "            return False  # Not a success\n",
    "\n",
    "        author = details.get('author', 'Unknown Author').strip()\n",
    "        title = details.get('title', 'Untitled').strip()\n",
    "        note = details.get('note', '').strip()\n",
    "\n",
    "        # Sanitize author and title for filenames\n",
    "        author_sanitized = sanitize_filename(author)\n",
    "        title_sanitized = sanitize_filename(title)\n",
    "        base_name = f\"{author_sanitized} -- {title_sanitized}\"\n",
    "        new_filename = handle_duplicate(target_dir, base_name)\n",
    "        new_zip_path = target_dir / new_filename\n",
    "\n",
    "        try:\n",
    "            # Copy and rename the zip file\n",
    "            shutil.copy2(zip_path, new_zip_path)\n",
    "            # Optional: Uncomment the line below to enable verbose copying logs\n",
    "            # print(f\"Copied and renamed '{zip_path.name}' to '{new_filename}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error copying '{zip_path}' to '{new_zip_path}': {e}\")\n",
    "            return False\n",
    "\n",
    "        # Update results\n",
    "        results[str(new_filename)] = {\n",
    "            'title': title,\n",
    "            'author': author,\n",
    "            'note': note,\n",
    "            'filename': new_filename\n",
    "        }\n",
    "\n",
    "        # Increment save counter and save if needed\n",
    "        save_counter['count'] += 1\n",
    "        if save_counter['count'] >= save_every:\n",
    "            save_results(results, results_file)\n",
    "            save_counter['count'] = 0  # Reset counter\n",
    "\n",
    "        return True  # Success\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to organize book zip files.\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    source_dir = Path(\"I:/gutenberg\").resolve()\n",
    "    target_dir = Path(\"I:/gutenberg_processed\").resolve()\n",
    "    results_file = Path(\"I:/gutenberg_processed/_gutenberg_index.json\").resolve()\n",
    "    save_every = 10  # Number of items to process before saving\n",
    "\n",
    "    if not source_dir.is_dir():\n",
    "        print(f\"Source directory '{source_dir}' does not exist or is not a directory.\")\n",
    "        return\n",
    "\n",
    "    target_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load existing results for resuming\n",
    "    results = load_results(results_file)\n",
    "\n",
    "    # Find all zip files\n",
    "    zip_files = find_zip_files(source_dir)\n",
    "    total_files = len(zip_files)\n",
    "    print(f\"Found {total_files} zip files in '{source_dir}'.\")\n",
    "\n",
    "    # Filter out already processed zip files\n",
    "    zip_files_to_process = [zf for zf in zip_files if str(zf) not in results]\n",
    "    remaining = len(zip_files_to_process)\n",
    "    print(f\"{remaining} zip files to process.\")\n",
    "\n",
    "    if remaining == 0:\n",
    "        print(\"No new zip files to process.\")\n",
    "        return\n",
    "\n",
    "    # Initialize progress bar\n",
    "    with tqdm(total=remaining, desc=\"Processing Zip Files\", unit=\"zip\") as pbar:\n",
    "        # Initialize a counter dictionary to keep track of items since last save\n",
    "        save_counter = {'count': 0}\n",
    "        for zip_path in zip_files_to_process:\n",
    "            try:\n",
    "                success = process_zip_file(zip_path, target_dir, results, results_file, save_every, save_counter)\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error processing '{zip_path}': {e}\")\n",
    "            pbar.update(1)\n",
    "\n",
    "        # After processing all files, save any remaining results\n",
    "        if save_counter['count'] > 0:\n",
    "            save_results(results, results_file)\n",
    "\n",
    "    print(\"All zip files processed.\")\n",
    "\n",
    "# ----------------------------\n",
    "# Entry Point\n",
    "# ----------------------------\n",
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
