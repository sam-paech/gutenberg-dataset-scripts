{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_prompt_prompt = \"\"\"\n",
    "Given the above chapter from <BOOK>, your task is to craft a prompt to elicit a similar chapter on essentially the same topic & contents as this chapter, without the model seeing its contents. Your prompt should describe the set-up for the chapter, the style, tone & setting, time period, the characters in generalities, and the names of the characters. It should not mention the author or title.\n",
    "\n",
    "The prompt should be one paragraph long. Do not add any additional commentary.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "\n",
    "api_key = \"\"\n",
    "openai_client = openai.OpenAI(\n",
    "    api_key=api_key,\n",
    "    #base_url=base_url\n",
    ")\n",
    "def run_openai_query(prompt, history, completion_tokens, temp, model, openai_client):\n",
    "    response = None\n",
    "    try:\n",
    "        messages = history + [{\"role\": \"user\", \"content\": prompt}]\n",
    "        #messages = history + [{\"role\": \"system\", \"content\": \"You must complete the entire writing piece in a single output, using multiple iterations of planning, drafting and outout. Do not stop writing until you hit the required word limit.\"}, {\"role\": \"user\", \"content\": prompt}]\n",
    "        \n",
    "        response = openai_client.chat.completions.create(\n",
    "                model=model,\n",
    "                temperature=temp,\n",
    "                max_tokens=completion_tokens,\n",
    "                #max_tokens=16000,\n",
    "                messages=messages,\n",
    "                #min_p = 0.1,\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "            \n",
    "        if content:\n",
    "            return content.strip()\n",
    "        else:\n",
    "            print(response)\n",
    "            print('Error: message is empty')\n",
    "            time.sleep(5)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(response)\n",
    "        print(\"Request failed.\")\n",
    "        print(e)\n",
    "        time.sleep(5)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [09:24<00:00,  8.83s/it]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Load scenes and existing prompts if available\n",
    "with open('data/site/processing/site_scenes_fiction_cleaned.json', 'r') as f:\n",
    "    scenes = json.load(f)\n",
    "\n",
    "outfile = \"data/site/processing/site_scenes_with_prompts_fiction.json\"\n",
    "prompts = {}\n",
    "if os.path.exists(outfile):\n",
    "    with open(outfile, 'r') as f:\n",
    "        prompts = json.load(f)\n",
    "\n",
    "# Function to process each scene\n",
    "def process_scene(scene, fn, book):\n",
    "    if len(scene) < 4000:\n",
    "        return None  # Skip scenes that are too short\n",
    "    if 'gutenberg' in scene.lower():\n",
    "        return None  # Skip scenes mentioning 'gutenberg'\n",
    "\n",
    "    # Create prompt and get response from OpenAI\n",
    "    prompt = create_prompt_prompt.replace('<BOOK>', book)\n",
    "    response = run_openai_query(prompt, [], 1024, 1, \"chatgpt-4o-latest\", openai_client)\n",
    "\n",
    "    # Return the processed scene\n",
    "    return {\n",
    "        \"source\": fn,\n",
    "        \"chosen\": scene,\n",
    "        \"prompt\": response\n",
    "    }\n",
    "\n",
    "# Main loop to iterate through files and scenes\n",
    "for fn, v in tqdm(scenes.items()):\n",
    "    if fn in prompts:\n",
    "        print('skipping', fn)\n",
    "        continue\n",
    "    \n",
    "    this_scenes = []\n",
    "    book = fn.strip('.zip')\n",
    "\n",
    "    # Use ThreadPoolExecutor for multithreading the inner loop\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(process_scene, scene, fn, book): scene for scene in v}\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result:  # If the result is not None (i.e., scene is valid)\n",
    "                this_scenes.append(result)\n",
    "\n",
    "    # Update prompts with processed scenes\n",
    "    prompts[fn] = this_scenes\n",
    "\n",
    "    # Save the updated prompts to file\n",
    "    with open(outfile, 'w') as f:\n",
    "        json.dump(prompts, f, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
